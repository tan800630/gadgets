{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from cnn_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 設定使用兩顆GPU執行程式，若只有一顆則此行不須執行 (等同於 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" )\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'configuration.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params from scratch\n",
    "\n",
    "params_dict = {'model_params':{},\n",
    "            'data_params':{},\n",
    "            'train_params':{}}\n",
    "\n",
    "params_dict['data_params']['df_dir'] = 'NIH_Chest_X_ray/Data_Entry_2017.csv' # 訓練資料的csv檔案\n",
    "params_dict['data_params']['img_dir'] = 'NIH_Chest_X_ray/images/images/' # 訓練資料的圖片資料夾位置\n",
    "params_dict['data_params']['file_col'] = 'Image Index'\n",
    "params_dict['data_params']['target_col'] = 'View Position'\n",
    "params_dict['data_params']['data_save_path'] = 'ResNet50_sample5000_preproc2_data.pkl'\n",
    "\n",
    "\n",
    "# parameters of present model\n",
    "params_dict['model_params']['pretrain_model'] = 'imagenet:ResNet50' # file_path or imagenet pretrain model\n",
    "params_dict['model_params']['model_dir'] = 'saved_models' # 模型存放位置\n",
    "params_dict['model_params']['model_name'] = 'ResNet50_can_be_deleted' # 儲存模型名稱(自由命名)\n",
    "params_dict['model_params']['weight_init'] = 'glorot_uniform'\n",
    "\n",
    "\n",
    "# parameters of training\n",
    "params_dict['train_params']['histogram_equalization'] = True\n",
    "params_dict['train_params']['scale_range'] = (-1,1)\n",
    "params_dict['train_params']['img_augmentation'] = True\n",
    "params_dict['train_params']['batch_size'] = 64\n",
    "params_dict['train_params']['epochs'] = 200\n",
    "params_dict['train_params']['img_shape'] = (224,224,3)\n",
    "params_dict['train_params']['weight_regularization'] = {'l1':0, 'l2':0}\n",
    "params_dict['train_params']['dropout_rate'] = 0.5\n",
    "params_dict['train_params']['early_stop_round'] = 5\n",
    "params_dict['train_params']['optimizer'] = {'optimizer':'Adam',\n",
    "                                            'momentum':None,\n",
    "                                            'lr' : 1e-5,\n",
    "                                            'decay': 0.0,  #SGD & RMSprop & Adagrad & Adam\n",
    "                                            'nesterov': None,  #SGD\n",
    "                                            'rho': None,  #RMSprop\n",
    "                                            'epsilon': None  #RMSprop & Adagrad \n",
    "                                           }\n",
    "params_dict['train_params']['loss'] = 'categorical_crossentropy'\n",
    "params_dict['train_params']['freeze_layer'] = None\n",
    "params_dict['train_params']['random_state'] = 1048\n",
    "\n",
    "\n",
    "with open('configuration.config', 'w') as f:\n",
    "    f.write(json.dumps(params_dict, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params from params_dict\n",
    "\n",
    "params_dict = json.load(open(config_path,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "切分訓練集、驗證集、測試集並將檔案存出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(params_dict['data_params']['df_dir']) is str:\n",
    "\n",
    "    data = pd.read_csv(params_dict['data_params']['df_dir'])\n",
    "\n",
    "    preproc = get_preproc_function(scale_range = params_dict['train_params']['scale_range'],\n",
    "                                   histogram_equalization = params_dict['train_params']['histogram_equalization'])\n",
    "\n",
    "    #setting\n",
    "\n",
    "    n_samples = 5000\n",
    "\n",
    "    # 取出2成資料id作為測試資料id\n",
    "    test_id = take_test_id(data, random_state = params_dict['train_params']['random_state'], target_col = params_dict['data_params']['target_col'])\n",
    "\n",
    "    train_dat = data.drop(test_id)\n",
    "    test_dat = data.loc[test_id]\n",
    "\n",
    "\n",
    "    # 從剩下的資料中取出 n_samples 筆資料作為訓練與驗證集\n",
    "    train_dat = train_dat[:n_samples]\n",
    "\n",
    "    # 打亂並再度切割資料作為訓練與驗證集 (8 : 2)\n",
    "    t_dat, v_dat = train_test_split(train_dat, shuffle = True, test_size = 0.2, random_state = params_dict['train_params']['random_state'])\n",
    "\n",
    "    print(t_dat.shape)\n",
    "    print(v_dat.shape)\n",
    "\n",
    "elif type(params_dict['data_params']['df_dir']) is dict:\n",
    "    \n",
    "    t_dat = params_dict['data_params']['df_dir']['training_set']\n",
    "    v_dat = params_dict['data_params']['df_dir']['validation_set']\n",
    "    test_dat = params_dict['data_params']['df_dir']['testing_set']\n",
    "    \n",
    "dat_dict = {'training_set': t_dat,\n",
    "            'validation_set': v_dat,\n",
    "            'testing_set': test_dat}\n",
    "\n",
    "\n",
    "# 將三份檔案儲存為pickle檔\n",
    "if params_dict['data_params']['data_save_path']!='':\n",
    "\n",
    "    pickle.dump(dat_dict,open(params_dict['data_params']['data_save_path'], 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 12)\n",
      "(1000, 12)\n"
     ]
    }
   ],
   "source": [
    "train_gen = image_data_generator(t_dat, img_dir = params_dict['data_params']['img_dir'],\n",
    "                                 file_col = params_dict['data_params']['file_col'],\n",
    "                                 target_col = params_dict['data_params']['target_col'],\n",
    "                                 batch_size = params_dict['train_params']['batch_size'],\n",
    "                                 re_size = params_dict['train_params']['img_shape'][0:2],\n",
    "                                 preprocess_function=preproc,\n",
    "                                 augmentation = params_dict['train_params']['img_augmentation'])\n",
    "\n",
    "valid_gen = image_data_generator(v_dat, img_dir = params_dict['data_params']['img_dir'],\n",
    "                                 file_col = params_dict['data_params']['file_col'],\n",
    "                                 target_col = params_dict['data_params']['target_col'],\n",
    "                                 batch_size = params_dict['train_params']['batch_size'],\n",
    "                                 re_size = params_dict['train_params']['img_shape'][0:2],\n",
    "                                 preprocess_function=preproc,\n",
    "                                 augmentation = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if '/' in params_dict['model_params']['pretrain_model']:\n",
    "    model = load_model(params_dict['model_params']['pretrain_model'])\n",
    "else:\n",
    "    # 使用ResNet50作為基本模型(可更換)\n",
    "    \n",
    "    imagenet_pretrain_model = params_dict['model_params']['pretrain_model'].split(':')[1]\n",
    "    \n",
    "    if imagenet_pretrain_model=='ResNet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=params_dict['train_params']['img_shape'])\n",
    "    elif imagenet_pretrain_model=='VGG16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=params_dict['train_params']['img_shape'])\n",
    "    elif imagenet_pretrain_model=='InceptionV3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=params_dict['train_params']['img_shape'])\n",
    "    elif imagenet_pretrain_model=='InceptionResNetV2':\n",
    "        base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=params_dict['train_params']['img_shape'])\n",
    "    elif imagenet_pretrain_model=='DenseNet201':\n",
    "        base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=params_dict['train_params']['img_shape'])\n",
    "    elif imagenet_pretrain_model=='NASNetLarge':\n",
    "        base_model = NASNetLarge(weights='imagenet', include_top=False, input_shape=params_dict['train_params']['img_shape'])\n",
    "    else:\n",
    "        print('parameter in config.model_params.pretrain_model should be either a path or imagenet:\\'pretain_model_name\\'')\n",
    "   \n",
    "    n_class = t_dat[params_dict['data_params']['target_col']].nunique()\n",
    "    \n",
    "    # layer freeze:\n",
    "    if params_dict['train_params']['freeze_layer']:\n",
    "        for i in params_dict['train_params']['freeze_layer'] :\n",
    "            base_model.layers[i].trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(params_dict['train_params']['dropout_rate'])(x)\n",
    "    \n",
    "    if params_dict['train_params']['weight_regularization']:\n",
    "        reg = keras.regularizers.l1_l2(l1 = params_dict['train_params']['weight_regularization']['l1'],\n",
    "                                               l2 = params_dict['train_params']['weight_regularization']['l2'])\n",
    "    else:\n",
    "        reg = None\n",
    "    predictions = Dense(n_class, activation='softmax', kernel_regularizer=reg)(x)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "if params_dict['train_params']['optimizer']['optimizer']=='SGD':\n",
    "    optimizer = keras.optimizers.SGD(lr=params_dict['train_params']['optimizer']['lr'],\n",
    "                                     momentum=params_dict['train_params']['optimizer']['momentum'],\n",
    "                                     decay=params_dict['train_params']['optimizer']['decay'],\n",
    "                                     nesterov=params_dict['train_params']['optimizer']['nesterov'])\n",
    "elif params_dict['train_params']['optimizer']['optimizer']=='RMSprop':\n",
    "    optimizer = keras.optimizers.RMSprop(lr=params_dict['train_params']['optimizer']['lr'],\n",
    "                                         rho=params_dict['train_params']['optimizer']['rho'],\n",
    "                                         epsilon=params_dict['train_params']['optimizer']['epsilon'],\n",
    "                                         decay=params_dict['train_params']['optimizer']['decay'])\n",
    "elif params_dict['train_params']['optimizer']['optimizer']=='Adagrad':\n",
    "    optimizer = keras.optimizers.Adagrad(lr=params_dict['train_params']['optimizer']['lr'],\n",
    "                                         epsilon=params_dict['train_params']['optimizer']['epsilon'],\n",
    "                                         decay=params_dict['train_params']['optimizer']['decay'])\n",
    "elif params_dict['train_params']['optimizer']['optimizer']=='Adam':\n",
    "    optimizer = keras.optimizers.Adam(lr=params_dict['train_params']['optimizer']['lr'],\n",
    "                                      epsilon=params_dict['train_params']['optimizer']['epsilon'],\n",
    "                                      decay=params_dict['train_params']['optimizer']['decay'])\n",
    "    \n",
    "model.compile(loss=params_dict['train_params']['loss'],\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/62 [============================>.] - ETA: 2s - loss: 0.7001 - acc: 0.7746"
     ]
    }
   ],
   "source": [
    "# 設定模型儲存的位置\n",
    "model_path = params_dict['model_params']['model_dir']+'/{}.h5'.format(params_dict['model_params']['model_name'])\n",
    "\n",
    "if params_dict['train_params']['early_stop_round']!=None:\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=params_dict['train_params']['early_stop_round'], verbose=1)\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    callbacks = [checkpoint, earlystop]\n",
    "else:\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='loss', save_best_only=True, verbose=1)\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "\n",
    "model_history = model.fit_generator(train_gen,\n",
    "                                    epochs = params_dict['train_params']['epochs'],\n",
    "                                    validation_data = valid_gen,\n",
    "                                    callbacks = callbacks,\n",
    "                                    verbose=1, steps_per_epoch = math.ceil(len(t_dat)/params_dict['train_params']['batch_size']),\n",
    "                                    validation_steps = math.ceil(len(v_dat)/params_dict['train_params']['batch_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整訓練完模型得到 model_history後，可以畫出訓練階段的狀況\n",
    "\n",
    "training_loss = model_history.history['loss']\n",
    "plt.plot(training_loss, label=\"training_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve\")\n",
    "\n",
    "try:\n",
    "    val_loss = model_history.history['val_loss']\n",
    "    plt.plot(val_loss, label=\"validation_loss\")\n",
    "    plt.legend(loc='best')\n",
    "except:\n",
    "    None\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing - log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load parameters\n",
    "\n",
    "params_dict = json.load(open(config_path,'r'))\n",
    "dat_dict = pickle.load(open(params_dict['data_params']['data_save_path'], 'rb'))\n",
    "\n",
    "# 載入已經訓練好的模型\n",
    "model = load_model(os.path.join(params_dict['model_params']['model_dir'], params_dict['model_params']['model_name']+'.h5'))\n",
    "\n",
    "preproc = get_preproc_function(scale_range = params_dict['train_params']['scale_range'],\n",
    "                               histogram_equalization = params_dict['train_params']['histogram_equalization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict['general'] = {}\n",
    "log_dict['general']['model_path'] = os.path.join(params_dict['model_params']['model_dir'], params_dict['model_params']['model_name']+'.h5')\n",
    "log_dict['general']['config_path'] = config_path\n",
    "\n",
    "for dat_type, dat in dat_dict.items():\n",
    "    \n",
    "    log_dict[dat_type] = {}\n",
    "    \n",
    "    gen = image_data_generator(dat, img_dir = params_dict['data_params']['img_dir'],\n",
    "                                     file_col = params_dict['data_params']['file_col'],\n",
    "                                     target_col = params_dict['data_params']['target_col'],\n",
    "                                     batch_size = params_dict['train_params']['batch_size'],\n",
    "                                     re_size = params_dict['train_params']['img_shape'][0:2],\n",
    "                                     preprocess_function=preproc,\n",
    "                                     augmentation = False, shuffle_ = False)\n",
    "    \n",
    "    true_ = np.argmax(pd.get_dummies(dat[params_dict['data_params']['target_col']]).values, axis = 1)\n",
    "    pred = model.predict_generator(gen, steps = math.ceil(len(dat)/params_dict['train_params']['batch_size']))\n",
    "\n",
    "    cm = confusion_matrix(y_true = true_, y_pred = np.argmax(pred, axis = 1))\n",
    "    acc = accuracy_score(y_true = true_, y_pred = np.argmax(pred, axis = 1))\n",
    "    precision = list(precision_score(y_true = true_, y_pred = np.argmax(pred, axis = 1), average = None)\n",
    "    recall = list(recall_score(y_true = true_, y_pred = np.argmax(pred, axis = 1), average = None)\n",
    "    \n",
    "    # save result to log_dict\n",
    "    log_dict[dat_type]['length'] = dat.shape[0]\n",
    "    log_dict[dat_type]['accuracy'] = acc\n",
    "    log_dict[dat_type]['confusion_matrix'] = cm.tolist()\n",
    "    log_dict[dat_type]['precision'] = precision\n",
    "    log_dict[dat_type]['recall'] = recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_name = params_dict['model_params']['model_name']+'.log'\n",
    "\n",
    "with open(log_name, 'w') as f:\n",
    "    f.write(json.dumps(log_dict, indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load parameters\n",
    "\n",
    "params_dict = json.load(open(config_path,'r'))\n",
    "\n",
    "# 載入已經訓練好的模型\n",
    "model = load_model(os.path.join(params_dict['model_params']['model_dir'], params_dict['model_params']['model_name']+'.h5'))\n",
    "\n",
    "preproc = get_preproc_function(scale_range = params_dict['train_params']['scale_range'],\n",
    "                               histogram_equalization = params_dict['train_params']['histogram_equalization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112120 images found in directory\n",
      "0 images had been processed.\n",
      "1000 images had been processed.\n",
      "2000 images had been processed.\n",
      "3000 images had been processed.\n",
      "4000 images had been processed.\n",
      "5000 images had been processed.\n",
      "6000 images had been processed.\n",
      "7000 images had been processed.\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "\n",
    "# 設定要做測試時的batch_size\n",
    "batch_size = 16\n",
    "\n",
    "# 設定要讀取圖片的 path\n",
    "test_image_path = 'NIH_Chest_X_ray/images/images/'\n",
    "\n",
    "# 設定要存出的檔案名稱\n",
    "out_file_name = 'test.csv'\n",
    "\n",
    "###########################################\n",
    "\n",
    "# test data generator\n",
    "test_gen = generator_from_dir(test_image_path,\n",
    "                              target_size = tuple(params_dict['train_params']['img_shape'][:2]),\n",
    "                              batch_size = batch_size,\n",
    "                              preprocess_function = preproc)\n",
    "\n",
    "\n",
    "test_prediction = []\n",
    "test_label = []\n",
    "file_list = []\n",
    "\n",
    "# testing data prediction\n",
    "for i, [image_batch, file_batch] in enumerate(test_gen):\n",
    "    prediction = model.predict(np.array(image_batch))\n",
    "\n",
    "    test_prediction.extend(prediction)\n",
    "    file_list.extend(file_batch)\n",
    "    \n",
    "    if i%10==0:\n",
    "        print('{} batchs had been processed.'.format(i))\n",
    "\n",
    "# 存出檔案\n",
    "out_file = pd.DataFrame({\n",
    "    'file_name' : file_list,\n",
    "    'prediction' : np.argmax(np.array(test_prediction), axis = 1),\n",
    "})\n",
    "    \n",
    "out_file.to_csv(out_file_name, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
